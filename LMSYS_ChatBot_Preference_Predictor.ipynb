{"cells":[{"cell_type":"markdown","source":["## LMSYS COMPETITION NOTEBOOK\n","\n","Forked From: https://www.kaggle.com/code/nabojyotipandey/dual-model-inference-gemma2-9b-llama3-8b and fine-tuned\n","\n","DATASETS LOADED IN ENVIRONMENT:\n","- LMSYS\n","- Llama3\n","- Gemma2\n","- bitsandbytes\n","\n","This project aims to predict user preferences in head-to-head battles between responses from large language models (LLMs). Utilizing data from the LMSYS ChatBot Arena Kaggle Competition, we employed a model ensemble approach combining two pre-trained models, Gemma2 and Llama3.\n"],"metadata":{"id":"BQEkIFjD9p16"},"id":"BQEkIFjD9p16"},{"cell_type":"code","execution_count":null,"id":"c569b03a","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-05T20:04:15.204607Z","iopub.status.busy":"2024-08-05T20:04:15.203818Z","iopub.status.idle":"2024-08-05T20:04:32.326811Z","shell.execute_reply":"2024-08-05T20:04:32.325678Z"},"papermill":{"duration":17.132644,"end_time":"2024-08-05T20:04:32.329198","exception":false,"start_time":"2024-08-05T20:04:15.196554","status":"completed"},"tags":[],"id":"c569b03a"},"outputs":[],"source":["!pip install transformers peft accelerate bitsandbytes -U --no-index --find-links /kaggle/input/lmsys-wheel-files"]},{"cell_type":"code","execution_count":null,"id":"2c47d3fa","metadata":{"execution":{"iopub.execute_input":"2024-08-05T20:04:32.360620Z","iopub.status.busy":"2024-08-05T20:04:32.359477Z","iopub.status.idle":"2024-08-05T20:04:50.862812Z","shell.execute_reply":"2024-08-05T20:04:50.861822Z"},"papermill":{"duration":18.514644,"end_time":"2024-08-05T20:04:50.865745","exception":false,"start_time":"2024-08-05T20:04:32.351101","status":"completed"},"tags":[],"id":"2c47d3fa"},"outputs":[],"source":["# IMPORTS\n","import time\n","from dataclasses import dataclass\n","from concurrent.futures import ThreadPoolExecutor\n","\n","import torch\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","from transformers import (\n","    Gemma2ForSequenceClassification, GemmaTokenizerFast,\n","    AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig\n",")\n","from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n","from peft import PeftModel, get_peft_model, LoraConfig, TaskType\n","\n","torch.backends.cuda.enable_mem_efficient_sdp(True)\n","torch.backends.cuda.enable_flash_sdp(True)"]},{"cell_type":"markdown","source":["## CONFIG"],"metadata":{"id":"UyDcBCaALZbA"},"id":"UyDcBCaALZbA"},{"cell_type":"code","execution_count":null,"id":"783b6c29","metadata":{"execution":{"iopub.execute_input":"2024-08-05T20:04:50.900710Z","iopub.status.busy":"2024-08-05T20:04:50.899510Z","iopub.status.idle":"2024-08-05T20:04:50.907417Z","shell.execute_reply":"2024-08-05T20:04:50.906528Z"},"papermill":{"duration":0.01823,"end_time":"2024-08-05T20:04:50.909262","exception":false,"start_time":"2024-08-05T20:04:50.891032","status":"completed"},"tags":[],"id":"783b6c29"},"outputs":[],"source":["# CONFIGURATION CLASS\n","@dataclass\n","class Config:\n","    gemma_dir: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n","    gemma_lora_dir: str = '/kaggle/input/73zap2gx/checkpoint-5748'\n","    llama_model_name: str = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n","    llama_weights_path: str = '/kaggle/input/lmsys-model/model'\n","    max_length: int = 2048\n","    batch_size: int = 4\n","    tta: bool = False\n","    spread_max_length: bool = False\n","\n","# Instantiate the configuration\n","cfg = Config()"]},{"cell_type":"markdown","source":["## LOAD DATA"],"metadata":{"id":"RAwH_apZLwXR"},"id":"RAwH_apZLwXR"},{"cell_type":"code","execution_count":null,"id":"2590a5af","metadata":{"execution":{"iopub.execute_input":"2024-08-05T20:04:50.940871Z","iopub.status.busy":"2024-08-05T20:04:50.940547Z","iopub.status.idle":"2024-08-05T20:04:50.962573Z","shell.execute_reply":"2024-08-05T20:04:50.961750Z"},"papermill":{"duration":0.033141,"end_time":"2024-08-05T20:04:50.964850","exception":false,"start_time":"2024-08-05T20:04:50.931709","status":"completed"},"tags":[],"id":"2590a5af"},"outputs":[],"source":["# LOAD AND PROCESS TEST DATA\n","test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n","\n","def process_text(text: str) -> str:\n","    return \" \".join(eval(text, {\"null\": \"\"}))\n","\n","test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n","test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n","test.loc[:, 'response_b'] = test['response_b'].apply(process_text)"]},{"cell_type":"markdown","source":["## TOKENIZE"],"metadata":{"id":"s8so6oI6Ldof"},"id":"s8so6oI6Ldof"},{"cell_type":"code","execution_count":null,"id":"f0651c60","metadata":{"execution":{"iopub.execute_input":"2024-08-05T20:04:51.007427Z","iopub.status.busy":"2024-08-05T20:04:51.006732Z","iopub.status.idle":"2024-08-05T20:04:51.016970Z","shell.execute_reply":"2024-08-05T20:04:51.016162Z"},"papermill":{"duration":0.021417,"end_time":"2024-08-05T20:04:51.019327","exception":false,"start_time":"2024-08-05T20:04:50.997910","status":"completed"},"tags":[],"id":"f0651c60"},"outputs":[],"source":["# TOKENIZE FUNCTION\n","def tokenize(tokenizer, prompt, response_a, response_b,\n","             max_length=cfg.max_length, spread_max_length=cfg.spread_max_length):\n","    # Handle different formats for different tokenizers\n","    if isinstance(tokenizer, GemmaTokenizerFast):\n","        prompt = [\"<prompt>: \" + p for p in prompt]\n","        response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n","        response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n","    else:\n","        prompt = [\"User prompt: \" + p for p in prompt]\n","        response_a = [\"\\n\\nModel A :\\n\" + r_a for r_a in response_a]\n","        response_b = [\"\\n\\n--------\\n\\nModel B:\\n\" + r_b for r_b in response_b]\n","\n","    # Tokenize with spread max length\n","    if spread_max_length:\n","        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n","        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n","        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n","        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n","        attention_mask = [[1] * len(i) for i in input_ids]\n","    # Tokenize without spread max length\n","    else:\n","        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n","        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n","        input_ids = tokenized.input_ids\n","        attention_mask = tokenized.attention_mask\n","\n","    return input_ids, attention_mask"]},{"cell_type":"code","execution_count":null,"id":"3107ab1b","metadata":{"execution":{"iopub.execute_input":"2024-08-05T20:04:51.053152Z","iopub.status.busy":"2024-08-05T20:04:51.052739Z","iopub.status.idle":"2024-08-05T20:04:52.475641Z","shell.execute_reply":"2024-08-05T20:04:52.474866Z"},"papermill":{"duration":1.43341,"end_time":"2024-08-05T20:04:52.477663","exception":false,"start_time":"2024-08-05T20:04:51.044253","status":"completed"},"tags":[],"id":"3107ab1b"},"outputs":[],"source":["# Gemma Tokenizer\n","gemma_tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n","gemma_tokenizer.add_eos_token = True\n","gemma_tokenizer.padding_side = \"right\"\n","\n","# Llama Tokenizer\n","llama_tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n","\n","# Prepare data for Gemma model\n","gemma_data = pd.DataFrame()\n","gemma_data[\"id\"] = test[\"id\"]\n","gemma_data[\"input_ids\"], gemma_data[\"attention_mask\"] = tokenize(gemma_tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n","gemma_data[\"length\"] = gemma_data[\"input_ids\"].apply(len)\n","\n","# Prepare data for Llama model\n","llama_data = pd.DataFrame()\n","llama_data[\"id\"] = test[\"id\"]\n","llama_data[\"input_ids\"], llama_data[\"attention_mask\"] = tokenize(llama_tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n","llama_data[\"length\"] = llama_data[\"input_ids\"].apply(len)"]},{"cell_type":"markdown","source":["## LOAD MODELS"],"metadata":{"id":"Dgn5mQJpLgOw"},"id":"Dgn5mQJpLgOw"},{"cell_type":"code","execution_count":null,"id":"42537832","metadata":{"execution":{"iopub.execute_input":"2024-08-05T20:04:52.508716Z","iopub.status.busy":"2024-08-05T20:04:52.508421Z","iopub.status.idle":"2024-08-05T20:06:04.185023Z","shell.execute_reply":"2024-08-05T20:06:04.183877Z"},"papermill":{"duration":71.687096,"end_time":"2024-08-05T20:06:04.187258","exception":false,"start_time":"2024-08-05T20:04:52.500162","status":"completed"},"tags":[],"id":"42537832"},"outputs":[],"source":["# Load Gemma model on GPU 0\n","device_0 = torch.device('cuda:0')\n","# Load the Gemma model for sequence classification onto GPU 0\n","gemma_model = Gemma2ForSequenceClassification.from_pretrained(\n","    cfg.gemma_dir,\n","    device_map=device_0,\n","    use_cache=False\n",")\n","# Apply PEFT using the LoRA checkpoint\n","gemma_model = PeftModel.from_pretrained(gemma_model, cfg.gemma_lora_dir)"]},{"cell_type":"code","execution_count":null,"id":"e32a1602","metadata":{"execution":{"iopub.execute_input":"2024-08-05T20:06:04.219747Z","iopub.status.busy":"2024-08-05T20:06:04.219131Z","iopub.status.idle":"2024-08-05T20:07:39.504861Z","shell.execute_reply":"2024-08-05T20:07:39.503990Z"},"papermill":{"duration":95.296513,"end_time":"2024-08-05T20:07:39.507074","exception":false,"start_time":"2024-08-05T20:06:04.210561","status":"completed"},"tags":[],"id":"e32a1602"},"outputs":[],"source":["# Load Llama model on GPU 1\n","device_1 = torch.device('cuda:1')\n","\n","# Configure BitsAndBytes for 8-bit loading\n","bnb_config = BitsAndBytesConfig(\n","    load_in_8bit=True,\n","    bnb_8bit_compute_dtype=torch.float16,\n","    bnb_8bit_use_double_quant=False\n",")\n","#  Load the Llama model for sequence classification onto GPU 1\n","llama_base_model = LlamaForSequenceClassification.from_pretrained(\n","    cfg.llama_model_name,\n","    num_labels=3,\n","    torch_dtype=torch.float16,\n","    quantization_config=bnb_config,\n","    device_map='cuda:1')\n","llama_base_model.config.pad_token_id = llama_tokenizer.pad_token_id\n","\n","# Configure and apply PEFT using the LoRA configuration\n","peft_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.10,\n","    bias='none',\n","    inference_mode=True,\n","    task_type=TaskType.SEQ_CLS,\n","    target_modules=['o_proj', 'v_proj']\n",")\n","llama_model = get_peft_model(llama_base_model, peft_config).to(device_1)\n","llama_model.load_state_dict(torch.load(cfg.llama_weights_path), strict=False)\n","llama_model.eval()"]},{"cell_type":"markdown","source":["## INFERENCE"],"metadata":{"id":"HlzQRvlXLiHW"},"id":"HlzQRvlXLiHW"},{"cell_type":"code","execution_count":null,"id":"b3f3e6e6","metadata":{"execution":{"iopub.execute_input":"2024-08-05T20:07:39.541552Z","iopub.status.busy":"2024-08-05T20:07:39.541263Z","iopub.status.idle":"2024-08-05T20:07:39.550728Z","shell.execute_reply":"2024-08-05T20:07:39.550070Z"},"papermill":{"duration":0.020743,"end_time":"2024-08-05T20:07:39.552674","exception":false,"start_time":"2024-08-05T20:07:39.531931","status":"completed"},"tags":[],"id":"b3f3e6e6"},"outputs":[],"source":["@torch.no_grad()\n","@torch.cuda.amp.autocast()\n","def inference(df, model, tokenizer, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n","    a_win, b_win, tie = [], [], []\n","    # Process the data in batches\n","    for start_idx in range(0, len(df), batch_size):\n","        end_idx = min(start_idx + batch_size, len(df))\n","        tmp = df.iloc[start_idx:end_idx]\n","        input_ids = tmp[\"input_ids\"].to_list()\n","        attention_mask = tmp[\"attention_mask\"].to_list()\n","        # Pad the inputs without triggering the fast tokenizer warning\n","        inputs = pad_without_fast_tokenizer_warning(\n","            tokenizer,\n","            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n","            padding=\"longest\",\n","            pad_to_multiple_of=None,\n","            return_tensors=\"pt\",\n","        )\n","        # Perform inference\n","        outputs = model(**inputs.to(device))\n","        proba = outputs.logits.softmax(-1).cpu()\n","\n","        # Append the probabilities to the corresponding lists\n","        a_win.extend(proba[:, 0].tolist())\n","        b_win.extend(proba[:, 1].tolist())\n","        tie.extend(proba[:, 2].tolist())\n","    # Update the DataFrame with the probabilities\n","    df[\"winner_model_a\"] = a_win\n","    df[\"winner_model_b\"] = b_win\n","    df[\"winner_tie\"] = tie\n","\n","    return df"]},{"cell_type":"markdown","source":["## VALIDATION TESTING AND WEIGHTING"],"metadata":{"id":"DDRo6iyDLmFw"},"id":"DDRo6iyDLmFw"},{"cell_type":"code","source":["# VALIDATION TESTING FOR MODEL WEIGHTING\n","\n","\"\"\"\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import log_loss\n","\n","# Load the training data\n","train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n","train.loc[:, 'prompt'] = train['prompt'].apply(process_text)\n","train.loc[:, 'response_a'] = train['response_a'].apply(process_text)\n","train.loc[:, 'response_b'] = train['response_b'].apply(process_text)\n","\n","# Prepare the labels for evaluation later\n","train[\"labels\"] = train.apply(lambda row: [row['winner_model_a'], row['winner_model_b'], row['winner_tie']], axis=1)\n","\n","# Split the dataset into training and validation sets\n","train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n","\n","# Tokenize the training data for each model\n","train_data_gemma = train_data.copy()\n","train_data_llama = train_data.copy()\n","\n","train_data_gemma[\"input_ids\"], train_data_gemma[\"attention_mask\"] = tokenize(\n","    gemma_tokenizer, train_data_gemma[\"prompt\"], train_data_gemma[\"response_a\"], train_data_gemma[\"response_b\"]\n",")\n","\n","train_data_llama[\"input_ids\"], train_data_llama[\"attention_mask\"] = tokenize(\n","    llama_tokenizer, train_data_llama[\"prompt\"], train_data_llama[\"response_a\"], train_data_llama[\"response_b\"]\n",")\n","\n","# Tokenize the validation data for each model\n","val_data_gemma = val_data.copy()\n","val_data_llama = val_data.copy()\n","\n","val_data_gemma[\"input_ids\"], val_data_gemma[\"attention_mask\"] = tokenize(\n","    gemma_tokenizer, val_data_gemma[\"prompt\"], val_data_gemma[\"response_a\"], val_data_gemma[\"response_b\"]\n",")\n","\n","val_data_llama[\"input_ids\"], val_data_llama[\"attention_mask\"] = tokenize(\n","    llama_tokenizer, val_data_llama[\"prompt\"], val_data_llama[\"response_a\"], val_data_llama[\"response_b\"]\n",")\n","\n","# Calculate the length of each input sequence for sorting\n","val_data_gemma[\"length\"] = val_data_gemma[\"input_ids\"].apply(len)\n","val_data_llama[\"length\"] = val_data_llama[\"input_ids\"].apply(len)\n","\n","# Sort by input length for efficient batching\n","val_data_gemma = val_data_gemma.sort_values(\"length\", ascending=False)\n","val_data_llama = val_data_llama.sort_values(\"length\", ascending=False)\n","\n","# Perform inference for both models using the correct devices\n","val_gemma_result_df = inference(val_data_gemma, gemma_model, gemma_tokenizer, device_0)\n","val_llama_result_df = inference(val_data_llama, llama_model, llama_tokenizer, device_1)\n","\n","# Calculate validation metrics (using log loss)\n","gemma_log_loss = log_loss(val_data['labels'].tolist(), val_gemma_result_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values)\n","llama_log_loss = log_loss(val_data['labels'].tolist(), val_llama_result_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values)\n","\n","# Print the log loss for both models\n","print(f\"Gemma Model Log Loss: {gemma_log_loss}\")\n","print(f\"Llama Model Log Loss: {llama_log_loss}\")\n","\n","# Calculate weights based on validation log loss\n","total_loss = gemma_log_loss + llama_log_loss\n","gemma_weight = llama_log_loss / total_loss\n","llama_weight = gemma_log_loss / total_loss\n","\n","print(f\"Calculated Weights - Gemma: {gemma_weight}, Llama: {llama_weight}\")\n","\"\"\"\n","# Gemma Weight was calculated as: 0.4257644179\n","# Llama Weight was calculated as: 0.5742355821"],"metadata":{"id":"yY_G5FYW60og"},"id":"yY_G5FYW60og","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GENERATE PREDICTIONS"],"metadata":{"id":"PHizMMiGLqIF"},"id":"PHizMMiGLqIF"},{"cell_type":"code","execution_count":null,"id":"3e521352","metadata":{"execution":{"iopub.execute_input":"2024-08-05T20:07:39.627643Z","iopub.status.busy":"2024-08-05T20:07:39.627168Z","iopub.status.idle":"2024-08-05T20:07:43.021833Z","shell.execute_reply":"2024-08-05T20:07:43.020980Z"},"papermill":{"duration":3.410511,"end_time":"2024-08-05T20:07:43.024194","exception":false,"start_time":"2024-08-05T20:07:39.613683","status":"completed"},"tags":[],"id":"3e521352"},"outputs":[],"source":["st = time.time()\n","\n","# Sort data by input length\n","gemma_data = gemma_data.sort_values(\"length\", ascending=False)\n","llama_data = llama_data.sort_values(\"length\", ascending=False)\n","\n","with ThreadPoolExecutor(max_workers=2) as executor:\n","    results = executor.map(inference,\n","                           (gemma_data, llama_data),\n","                           (gemma_model, llama_model),\n","                           (gemma_tokenizer, llama_tokenizer),\n","                           (device_0, device_1))\n","\n","gemma_result_df, llama_result_df = list(results)\n","\n","# Apply the weighted average to combine results\n","combined_result_df = gemma_result_df.copy()\n","combined_result_df[\"winner_model_a\"] = (gemma_weight * gemma_result_df[\"winner_model_a\"] + llama_weight * llama_result_df[\"winner_model_a\"])\n","combined_result_df[\"winner_model_b\"] = (gemma_weight * gemma_result_df[\"winner_model_b\"] + llama_weight * llama_result_df[\"winner_model_b\"])\n","combined_result_df[\"winner_tie\"] = (gemma_weight * gemma_result_df[\"winner_tie\"] + llama_weight * llama_result_df[\"winner_tie\"])"]},{"cell_type":"code","execution_count":null,"id":"4a28ae7a","metadata":{"execution":{"iopub.execute_input":"2024-08-05T20:07:43.060537Z","iopub.status.busy":"2024-08-05T20:07:43.060225Z","iopub.status.idle":"2024-08-05T20:07:43.079266Z","shell.execute_reply":"2024-08-05T20:07:43.078267Z"},"papermill":{"duration":0.030492,"end_time":"2024-08-05T20:07:43.081278","exception":false,"start_time":"2024-08-05T20:07:43.050786","status":"completed"},"tags":[],"id":"4a28ae7a"},"outputs":[],"source":["submission_df = combined_result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n","submission_df.to_csv('submission.csv', index=False)\n","display(submission_df.head())"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8346466,"sourceId":66631,"sourceType":"competition"},{"datasetId":5034873,"sourceId":8449074,"sourceType":"datasetVersion"},{"datasetId":5297895,"sourceId":8897601,"sourceType":"datasetVersion"},{"datasetId":5369301,"sourceId":8926343,"sourceType":"datasetVersion"},{"modelId":39106,"modelInstanceId":28083,"sourceId":33551,"sourceType":"modelInstanceVersion"},{"modelId":86587,"modelInstanceId":63082,"sourceId":75103,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":214.224957,"end_time":"2024-08-05T20:07:46.704213","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-08-05T20:04:12.479256","version":"2.5.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}